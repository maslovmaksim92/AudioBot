import os
import asyncio
from typing import Dict, Any, List, Optional
from loguru import logger
import httpx
from dotenv import load_dotenv
from emergentintegrations.llm.chat import LlmChat, UserMessage

# Load environment variables
load_dotenv()

class AIService:
    """AI Service using Emergent LLM integration for smart responses"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model = "gpt-4o-mini"  # Default model
        self.provider = "openai"
        self.system_message = """–¢—ã - AI-–ø–æ–º–æ—â–Ω–∏–∫ –∫–æ–º–ø–∞–Ω–∏–∏ –í–∞—Å–î–æ–º, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è:
- –£–±–æ—Ä–∫–æ–π –ø–æ–¥—ä–µ–∑–¥–æ–≤ –∏ –ø—Ä–∏–¥–æ–º–æ–≤—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å—é  
- –ö–ª–∏–Ω–∏–Ω–≥–æ–≤—ã–º–∏ —É—Å–ª—É–≥–∞–º–∏
- –†–∞–±–æ—Ç–æ–π —Å –ñ–ö–•

–û—Ç–≤–µ—á–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –ø–æ-—Ä—É—Å—Å–∫–∏. –ü–æ–º–æ–≥–∞–π –∫–ª–∏–µ–Ω—Ç–∞–º —Å:
- –í–æ–ø—Ä–æ—Å–∞–º–∏ –æ–± —É—Å–ª—É–≥–∞—Ö
- –ó–∞–ø–∏—Å—å—é –Ω–∞ —É–±–æ—Ä–∫—É
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ü–µ–Ω–∞—Ö
- –†–µ—à–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º

–ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º."""
        
        if not api_key:
            logger.warning("‚ö†Ô∏è AI Service initialized without API key")
    
    async def health_check(self) -> Dict[str, Any]:
        """Check AI service health"""
        if not self.api_key:
            return {"status": "not_configured", "api_key": False}
        
        try:
            # Test with a simple request
            response = await self.generate_response("–¢–µ—Å—Ç")
            return {
                "status": "healthy",
                "api_key": True,
                "model": self.model,
                "provider": self.provider,
                "test_response_length": len(response)
            }
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def generate_response(self, user_message: str, context: str = "") -> str:
        """Generate AI response for user message"""
        if not self.api_key:
            return "ü§ñ AI —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        
        try:
            # Create system message with context
            system_msg = self.system_message
            if context:
                system_msg += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}"
            
            # Initialize chat
            chat = LlmChat(
                api_key=self.api_key,
                session_id=f"vasdom_chat_{hash(user_message)}",
                system_message=system_msg
            ).with_model(self.provider, self.model)
            
            # Create user message
            message = UserMessage(text=user_message)
            
            # Get response
            response = await chat.send_message(message)
            
            if response and hasattr(response, 'text'):
                return response.text
            elif isinstance(response, str):
                return response
            else:
                return str(response)
                
        except Exception as e:
            logger.error(f"‚ùå AI generation error: {e}")
            return f"ü§ñ –ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –º–µ–Ω–µ–¥–∂–µ—Ä—É."
    
    async def generate_smart_reply(self, user_message: str, user_data: Dict = None) -> str:
        """Generate context-aware smart reply"""
        
        # Build context from user data
        context_parts = []
        if user_data:
            if user_data.get("name"):
                context_parts.append(f"–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_data['name']}")
            if user_data.get("phone"):
                context_parts.append(f"–¢–µ–ª–µ—Ñ–æ–Ω: {user_data['phone']}")
            if user_data.get("address"):
                context_parts.append(f"–ê–¥—Ä–µ—Å: {user_data['address']}")
            if user_data.get("previous_requests"):
                context_parts.append(f"–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –æ–±—Ä–∞—â–µ–Ω–∏—è: {user_data['previous_requests']}")
        
        context = "; ".join(context_parts) if context_parts else ""
        
        return await self.generate_response(user_message, context)
    
    async def analyze_user_intent(self, message: str) -> Dict[str, Any]:
        """Analyze user intent from message"""
        if not self.api_key:
            return {"intent": "unknown", "confidence": 0.0}
        
        try:
            analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏: "{message}"

–û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é:
- cleaning_request: —Ö–æ—á–µ—Ç –∑–∞–∫–∞–∑–∞—Ç—å —É–±–æ—Ä–∫—É
- price_inquiry: —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —Ü–µ–Ω–∞—Ö
- complaint: –∂–∞–ª–æ–±–∞ –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞  
- general_info: –æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- support_request: –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å –º–µ–Ω–µ–¥–∂–µ—Ä–∞
- other: –¥—Ä—É–≥–æ–µ

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –∫–∞—Ç–µ–≥–æ—Ä–∏—è|—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å(0-1)|–∫—Ä–∞—Ç–∫–æ–µ_–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"""
            
            response = await self.generate_response(analysis_prompt)
            
            # Parse response
            parts = response.split("|")
            if len(parts) >= 3:
                return {
                    "intent": parts[0].strip(),
                    "confidence": float(parts[1].strip()),
                    "explanation": parts[2].strip()
                }
            else:
                return {"intent": "other", "confidence": 0.5, "explanation": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å"}
                
        except Exception as e:
            logger.error(f"‚ùå Intent analysis error: {e}")
            return {"intent": "unknown", "confidence": 0.0, "explanation": str(e)}
    
    async def generate_summary(self, conversation: List[Dict]) -> str:
        """Generate conversation summary"""
        if not self.client or not conversation:
            return "–ö—Ä–∞—Ç–∫–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä –±–µ–∑ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π"
        
        try:
            # Format conversation
            conv_text = "\n".join([
                f"{msg.get('sender', 'User')}: {msg.get('text', '')}"
                for msg in conversation[-10:]  # Last 10 messages
            ])
            
            summary_prompt = f"""–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —Å –∫–ª–∏–µ–Ω—Ç–æ–º:

{conv_text}

–£–∫–∞–∂–∏:
- –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–∞
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
- –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
- –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞—â–µ–Ω–∏—è

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ, –¥–æ 200 —Å–ª–æ–≤."""
            
            return await self.generate_response(summary_prompt)
            
        except Exception as e:
            logger.error(f"‚ùå Summary generation error: {e}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑—é–º–µ: {str(e)}"