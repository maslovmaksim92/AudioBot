"""
Cron –∑–∞–¥–∞—á–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ AI
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional
from sqlalchemy import func, and_, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from app.config.database import SessionLocal
from app.config.settings import get_settings
from app.models.database import VoiceLogDB, ModelMetricsDB, TrainingDatasetDB
from training.export_logs import TrainingDataExporter
from training.fine_tune import fine_tune_from_export

logger = logging.getLogger(__name__)
settings = get_settings()

class ModelEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.evaluation_period_days = settings.EVALUATION_SCHEDULE_DAYS
        self.retraining_threshold = settings.RETRAINING_THRESHOLD
        self.min_interactions_for_evaluation = 100
    
    async def evaluate_current_model(self) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º —Ä–µ–π—Ç–∏–Ω–≥–∞–º
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        try:
            logger.info("üîç –ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            end_date = datetime.utcnow()
            start_date = end_date - timedelta(days=self.evaluation_period_days)
            
            async with SessionLocal() as db:
                # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥
                period_logs = db.query(VoiceLogDB).filter(
                    and_(
                        VoiceLogDB.created_at >= start_date,
                        VoiceLogDB.created_at <= end_date
                    )
                )
                
                total_interactions = await period_logs.count()
                
                # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏
                rated_logs = period_logs.filter(VoiceLogDB.rating.isnot(None))
                rated_count = await rated_logs.count()
                
                if rated_count < self.min_interactions_for_evaluation:
                    logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {rated_count} < {self.min_interactions_for_evaluation}")
                    return {
                        "success": False,
                        "reason": "insufficient_data",
                        "rated_interactions": rated_count,
                        "min_required": self.min_interactions_for_evaluation
                    }
                
                # –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥
                avg_rating_result = await db.query(
                    func.avg(VoiceLogDB.rating)
                ).filter(
                    and_(
                        VoiceLogDB.created_at >= start_date,
                        VoiceLogDB.created_at <= end_date,
                        VoiceLogDB.rating.isnot(None)
                    )
                ).scalar()
                
                avg_rating = float(avg_rating_result) if avg_rating_result else 0.0
                
                # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
                positive_ratings = await db.query(func.count(VoiceLogDB.id)).filter(
                    and_(
                        VoiceLogDB.created_at >= start_date,
                        VoiceLogDB.created_at <= end_date,
                        VoiceLogDB.rating >= 4
                    )
                ).scalar()
                
                negative_ratings = await db.query(func.count(VoiceLogDB.id)).filter(
                    and_(
                        VoiceLogDB.created_at >= start_date,
                        VoiceLogDB.created_at <= end_date,
                        VoiceLogDB.rating <= 2
                    )
                ).scalar()
                
                # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                user_satisfaction = positive_ratings / rated_count if rated_count > 0 else 0.0
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
                current_model_query = await db.query(VoiceLogDB.model_used).filter(
                    VoiceLogDB.created_at >= start_date
                ).distinct().all()
                
                current_models = [result[0] for result in current_model_query]
                primary_model = current_models[0] if current_models else "unknown"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏
                evaluation_result = {
                    "success": True,
                    "model_version": primary_model,
                    "evaluation_period": {
                        "start": start_date.isoformat(),
                        "end": end_date.isoformat(),
                        "days": self.evaluation_period_days
                    },
                    "metrics": {
                        "avg_rating": avg_rating,
                        "total_interactions": total_interactions,
                        "rated_interactions": rated_count,
                        "positive_ratings": positive_ratings,
                        "negative_ratings": negative_ratings,
                        "user_satisfaction": user_satisfaction,
                        "rating_coverage": rated_count / total_interactions if total_interactions > 0 else 0.0
                    },
                    "quality_assessment": {
                        "overall_quality": self._assess_quality(avg_rating, user_satisfaction),
                        "requires_retraining": avg_rating < self.retraining_threshold,
                        "threshold_used": self.retraining_threshold
                    },
                    "evaluated_at": datetime.utcnow().isoformat()
                }
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ –ë–î
                await self._save_evaluation_metrics(db, evaluation_result)
                
                logger.info(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {avg_rating:.2f}")
                
                return evaluation_result
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "evaluated_at": datetime.utcnow().isoformat()
            }
    
    def _assess_quality(self, avg_rating: float, user_satisfaction: float) -> str:
        """–û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        if avg_rating >= 4.0 and user_satisfaction >= 0.7:
            return "excellent"
        elif avg_rating >= 3.5 and user_satisfaction >= 0.6:
            return "good"
        elif avg_rating >= 3.0 and user_satisfaction >= 0.5:
            return "acceptable"
        elif avg_rating >= 2.5:
            return "poor"
        else:
            return "very_poor"
    
    async def _save_evaluation_metrics(self, db, evaluation_result: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            metrics = evaluation_result["metrics"]
            quality = evaluation_result["quality_assessment"]
            
            # –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            await db.query(ModelMetricsDB).filter(
                ModelMetricsDB.is_current_model == True
            ).update({"is_current_model": False})
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫
            new_metrics = ModelMetricsDB(
                model_version=evaluation_result["model_version"],
                avg_rating=metrics["avg_rating"],
                total_interactions=metrics["total_interactions"],
                positive_ratings=metrics["positive_ratings"],
                negative_ratings=metrics["negative_ratings"],
                user_satisfaction=metrics["user_satisfaction"],
                evaluation_period_start=datetime.fromisoformat(evaluation_result["evaluation_period"]["start"]),
                evaluation_period_end=datetime.fromisoformat(evaluation_result["evaluation_period"]["end"]),
                is_current_model=True,
                requires_retraining=quality["requires_retraining"]
            )
            
            db.add(new_metrics)
            await db.commit()
            
            logger.info(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏ {evaluation_result['model_version']}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {str(e)}")
    
    async def check_and_trigger_retraining(self) -> Dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        """
        try:
            logger.info("üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É
            evaluation_result = await self.evaluate_current_model()
            
            if not evaluation_result["success"]:
                return {
                    "action": "evaluation_failed",
                    "reason": evaluation_result.get("reason", "unknown"),
                    "details": evaluation_result
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
            quality_assessment = evaluation_result["quality_assessment"]
            
            if not quality_assessment["requires_retraining"]:
                logger.info(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è. –†–µ–π—Ç–∏–Ω–≥: {evaluation_result['metrics']['avg_rating']:.2f}")
                return {
                    "action": "no_retraining_needed",
                    "current_rating": evaluation_result["metrics"]["avg_rating"],
                    "threshold": quality_assessment["threshold_used"],
                    "evaluation": evaluation_result
                }
            
            logger.warning(f"‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ! –†–µ–π—Ç–∏–Ω–≥: {evaluation_result['metrics']['avg_rating']:.2f} < {quality_assessment['threshold_used']}")
            
            # –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            exporter = TrainingDataExporter(min_rating=settings.MIN_RATING_THRESHOLD)
            export_result = await exporter.export_training_data(
                days_back=30,  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
                max_samples=5000  # –ú–∞–∫—Å–∏–º—É–º 5000 –æ–±—Ä–∞–∑—Ü–æ–≤
            )
            
            if not export_result["success"]:
                return {
                    "action": "export_failed",
                    "error": export_result["error"],
                    "evaluation": evaluation_result
                }
            
            # –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            if export_result["filtered_samples"] >= 50:  # –ú–∏–Ω–∏–º—É–º 50 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
                logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —Å {export_result['filtered_samples']} –æ–±—Ä–∞–∑—Ü–∞–º–∏")
                
                retraining_result = await fine_tune_from_export(
                    jsonl_file_path=export_result["export_path"],
                    num_epochs=2,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
                    batch_size=4,
                    learning_rate=3e-5
                )
                
                return {
                    "action": "retraining_completed" if retraining_result["success"] else "retraining_failed",
                    "export": export_result,
                    "retraining": retraining_result,
                    "evaluation": evaluation_result
                }
            else:
                logger.warning(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {export_result['filtered_samples']} < 50")
                return {
                    "action": "insufficient_training_data",
                    "available_samples": export_result["filtered_samples"],
                    "min_required": 50,
                    "export": export_result,
                    "evaluation": evaluation_result
                }
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {str(e)}")
            return {
                "action": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }

# –ì–ª–∞–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è cron –∑–∞–¥–∞—á
async def evaluate_model():
    """Cron –∑–∞–¥–∞—á–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏"""
    logger.info("üìä –ó–∞–ø—É—Å–∫ cron –∑–∞–¥–∞—á–∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏")
    
    evaluator = ModelEvaluator()
    result = await evaluator.evaluate_current_model()
    
    if result["success"]:
        logger.info(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {result['metrics']['avg_rating']:.2f}")
    else:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {result.get('error', 'unknown')}")
    
    return result

async def check_retraining():
    """Cron –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    logger.info("üîÑ –ó–∞–ø—É—Å–∫ cron –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
    
    evaluator = ModelEvaluator()
    result = await evaluator.check_and_trigger_retraining()
    
    logger.info(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {result['action']}")
    
    return result

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python cron_tasks.py [evaluate_model|check_retraining]")
        sys.exit(1)
    
    task = sys.argv[1]
    
    if task == "evaluate_model":
        result = asyncio.run(evaluate_model())
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏: {result}")
    elif task == "check_retraining":
        result = asyncio.run(check_retraining())
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {result}")
    else:
        print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞: {task}")
        sys.exit(1)