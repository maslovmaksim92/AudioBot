"""
VasDom AudioBot - –°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è AI –¥–ª—è –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–µ–π
Production-ready –≤–µ—Ä—Å–∏—è –¥–ª—è Render —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import os
import json
import asyncio
from datetime import datetime, timedelta
import uuid
import hashlib
import numpy as np
from collections import deque
from dotenv import load_dotenv
import websockets
import time

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
import time

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–ø—É—Å–∫–∞
app_start_time = time.time()

# Prometheus –º–µ—Ç—Ä–∏–∫–∏
REQUEST_COUNT = Counter('vasdom_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('vasdom_request_duration_seconds', 'Request duration')
AI_RESPONSES = Counter('vasdom_ai_responses_total', 'AI responses generated', ['status'])
LEARNING_FEEDBACK = Counter('vasdom_learning_feedback_total', 'Learning feedback received', ['rating'])

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
from loguru import logger
import sys

# –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π handler –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–π
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO",
    serialize=False
)

# –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª –¥–ª—è production
logger.add(
    "/var/log/vasdom_audiobot.log",
    rotation="10 MB",
    retention="7 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}",
    catch=True
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ HTTP –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–µ–∞–ª—å–Ω—ã–º fallback
try:
    import aiohttp
    HTTP_CLIENT_AVAILABLE = True
    logger.success("‚úÖ aiohttp –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è HTTP API")
except ImportError:
    HTTP_CLIENT_AVAILABLE = False
    logger.warning("‚ùå aiohttp –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º requests fallback")
    try:
        import requests
        REQUESTS_AVAILABLE = True
        logger.success("‚úÖ requests fallback –¥–æ—Å—Ç—É–ø–µ–Ω")
    except ImportError:
        REQUESTS_AVAILABLE = False
        logger.error("‚ùå –ù–∏–∫–∞–∫–∏—Ö HTTP –∫–ª–∏–µ–Ω—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç–Ω–æ!")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
try:
    database_url = os.getenv("DATABASE_URL", "")
    if database_url and database_url.startswith("postgresql"):
        DATABASE_AVAILABLE = True
        logger.success(f"‚úÖ PostgreSQL –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç—É–ø–Ω–∞: {database_url.split('@')[1] if '@' in database_url else 'configured'}")
    else:
        DATABASE_AVAILABLE = False
        logger.info("üíæ PostgreSQL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º in-memory —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
except Exception as e:
    DATABASE_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
    logger.info("üíæ Fallback –Ω–∞ in-memory —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏")

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =============================================================================

class Config:
    # AI –∏ –æ–±—É—á–µ–Ω–∏–µ
    EMERGENT_LLM_KEY = os.getenv("EMERGENT_LLM_KEY", "")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L6-v2")
    MIN_RATING_THRESHOLD = int(os.getenv("MIN_RATING_THRESHOLD", "4"))
    RETRAINING_THRESHOLD = float(os.getenv("RETRAINING_THRESHOLD", "3.5"))
    
    # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
    DATABASE_URL = os.getenv("DATABASE_URL", "")
    
    # CORS
    CORS_ORIGINS = os.getenv("CORS_ORIGINS", "*").split(",")

config = Config()

# =============================================================================
# –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–•
# =============================================================================

class VoiceMessage(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000)
    session_id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()))

class VoiceResponse(BaseModel):
    response: str
    log_id: str
    session_id: str
    model_used: str = "gpt-4o-mini"
    response_time: float
    similar_found: int = 0
    learning_improved: bool = False

class FeedbackRequest(BaseModel):
    log_id: str
    rating: int = Field(..., ge=1, le=5)
    feedback_text: Optional[str] = None

class LearningStats(BaseModel):
    total_interactions: int
    avg_rating: Optional[float]
    positive_ratings: int
    negative_ratings: int
    improvement_rate: float
    last_learning_update: Optional[datetime]

class SimilarConversationResponse(BaseModel):
    log_id: str
    user_message: str
    ai_response: str
    similarity_score: float
    rating: Optional[int] = None

class RealtimeTokenResponse(BaseModel):
    token: str
    expires_at: int

class MeetingAnalysisRequest(BaseModel):
    transcription: str
    meeting_title: str
    duration: int

class MeetingAnalysisResponse(BaseModel):
    summary: str
    tasks: List[Dict[str, Any]]
    participants: List[str]

class BitrixTaskRequest(BaseModel):
    tasks: List[Dict[str, Any]]
    meeting_title: str
    meeting_summary: str
    participants: List[str]

# =============================================================================
# –ê–î–ê–ü–¢–ï–† –•–†–ê–ù–ò–õ–ò–©–ê
# =============================================================================

from storage_adapter import StorageAdapter

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–µ—Ä —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–µ—Ä–µ—Ç PostgreSQL –∏–ª–∏ in-memory)
storage = StorageAdapter()

# =============================================================================
# AI –°–ï–†–í–ò–° –° –†–ï–ê–õ–¨–ù–´–ú –°–ê–ú–û–û–ë–£–ß–ï–ù–ò–ï–ú
# =============================================================================

class SuperLearningAI:
    def __init__(self):
        self.llm_client = None
        self.learning_cache = {}
        self.last_training = None
        self.training_in_progress = False
        self.init_services()
    
    def init_services(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
        # Emergent LLM - –ø—Ä—è–º–∞—è HTTP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (–±–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏)
        if config.EMERGENT_LLM_KEY:
            try:
                # –°–æ–∑–¥–∞–µ–º –ø—Ä—è–º–æ–π HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è Emergent API
                class DirectEmergentLLM:
                    def __init__(self, api_key):
                        self.api_key = api_key
                        self.base_url = "https://api.emergent.ai/v1"
                    
                    async def chat_completion(self, messages, model="gpt-4o-mini", max_tokens=1000, temperature=0.7):
                        if HTTP_CLIENT_AVAILABLE:
                            return await self._aiohttp_request(messages, model, max_tokens, temperature)
                        elif REQUESTS_AVAILABLE:
                            return await self._requests_fallback(messages, model, max_tokens, temperature)
                        else:
                            raise Exception("No HTTP client available")
                    
                    async def _aiohttp_request(self, messages, model, max_tokens, temperature):
                        import aiohttp
                        try:
                            async with aiohttp.ClientSession() as session:
                                headers = {
                                    "Authorization": f"Bearer {self.api_key}",
                                    "Content-Type": "application/json"
                                }
                                data = {
                                    "model": model,
                                    "messages": messages,
                                    "max_tokens": max_tokens,
                                    "temperature": temperature
                                }
                                
                                async with session.post(f"{self.base_url}/chat/completions", 
                                                       headers=headers, json=data, timeout=30) as resp:
                                    if resp.status == 200:
                                        result = await resp.json()
                                        return self._create_response(result['choices'][0]['message']['content'])
                                    else:
                                        error_text = await resp.text()
                                        raise Exception(f"Emergent API error {resp.status}: {error_text}")
                        except Exception as e:
                            logger.error(f"Emergent API request failed: {e}")
                            raise e
                    
                    async def _requests_fallback(self, messages, model, max_tokens, temperature):
                        import requests
                        import asyncio
                        
                        def sync_request():
                            headers = {
                                "Authorization": f"Bearer {self.api_key}",
                                "Content-Type": "application/json"
                            }
                            data = {
                                "model": model,
                                "messages": messages,
                                "max_tokens": max_tokens,
                                "temperature": temperature
                            }
                            
                            resp = requests.post(f"{self.base_url}/chat/completions", 
                                               headers=headers, json=data, timeout=30)
                            if resp.status_code == 200:
                                result = resp.json()
                                return result['choices'][0]['message']['content']
                            else:
                                raise Exception(f"Emergent API error {resp.status_code}: {resp.text}")
                        
                        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å async
                        loop = asyncio.get_event_loop()
                        content = await loop.run_in_executor(None, sync_request)
                        return self._create_response(content)
                    
                    def _create_response(self, content):
                        class Choice:
                            def __init__(self, content):
                                self.message = type('obj', (object,), {'content': content})
                        
                        class Response:
                            def __init__(self, content):
                                self.choices = [Choice(content)]
                        
                        return Response(content)
                
                self.llm_client = DirectEmergentLLM(config.EMERGENT_LLM_KEY)
                logger.info("‚úÖ Emergent LLM —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π HTTP API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Emergent LLM: {e}")
                logger.info("üîÑ –†–∞–±–æ—Ç–∞–µ–º –≤ —Ä–µ–∂–∏–º–µ —É–º–Ω–æ–≥–æ fallback –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö LLM")
        else:
            logger.info("üîÑ EMERGENT_LLM_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–µ–∂–∏–º")
        
        # Embedding –º–æ–¥–µ–ª—å - —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ fallback (–±–µ–∑ sentence-transformers)
        logger.info("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback TF-IDF —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏")
    
    def create_embedding(self, text: str) -> Optional[np.ndarray]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π fallback –Ω–∞ TF-IDF)"""
        try:
            import hashlib
            # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤ –∏ –∏—Ö –ø–æ–∑–∏—Ü–∏–π
            words = text.lower().split()
            vector = np.zeros(384, dtype=np.float32)  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            
            for i, word in enumerate(words[:50]):  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 —Å–ª–æ–≤
                word_hash = int(hashlib.md5(word.encode()).hexdigest(), 16)
                vector[word_hash % 384] += 1.0 / (i + 1)  # –í–µ—Å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ–∑–∏—Ü–∏–∏
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
                
            return vector
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None
    
    def find_similar_conversations(self, query_text: str, limit: int = 3) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º)"""
        query_embedding = self.create_embedding(query_text)
        
        if query_embedding is None:
            # Fallback: –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ in-memory storage
            if hasattr(storage, 'conversations'):
                query_words = set(query_text.lower().split())
                similarities = []
                
                for conv in storage.conversations:
                    if conv.get("rating") is not None and conv.get("rating", 0) >= config.MIN_RATING_THRESHOLD:
                        conv_words = set(conv["user_message"].lower().split())
                        # Jaccard similarity (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ / –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ)
                        intersection = len(query_words & conv_words)
                        union = len(query_words | conv_words)
                        similarity = intersection / union if union > 0 else 0
                        
                        if similarity > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                            similarities.append((similarity, conv))
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
                similarities.sort(key=lambda x: x[0], reverse=True)
                return [conv for _, conv in similarities[:limit]]
            else:
                # –î–ª—è PostgreSQL —Ä–µ–∂–∏–º–∞ –ø–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                return []
        
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è in-memory)
        if hasattr(storage, 'conversations'):
            similarities = []
            for conv in storage.conversations:
                if conv.get("rating") is not None and conv.get("rating", 0) >= config.MIN_RATING_THRESHOLD:
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    conv_id = conv["log_id"]
                    conv_embedding = storage.load_embedding_safe(conv_id) if hasattr(storage, 'load_embedding_safe') else None
                    
                    if conv_embedding is None:
                        # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
                        conv_embedding = self.create_embedding(conv["user_message"])
                        if conv_embedding is not None and hasattr(storage, 'store_embedding_safe'):
                            storage.store_embedding_safe(conv_id, conv_embedding)
                    
                    if conv_embedding is not None:
                        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                        similarity = np.dot(query_embedding, conv_embedding) / (
                            np.linalg.norm(query_embedding) * np.linalg.norm(conv_embedding)
                        )
                        similarities.append((similarity, conv))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarities.sort(key=lambda x: x[0], reverse=True)
            return [conv for _, conv in similarities[:limit]]
        else:
            # –î–ª—è PostgreSQL —Ä–µ–∂–∏–º–∞ –ø–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
    
    def build_learning_prompt(self, user_message: str, similar_convs: List[Dict]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –ø–æ—Ö–æ–∂–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
        base_prompt = """–¢—ã - AI –ø–æ–º–æ—â–Ω–∏–∫ VasDom –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–µ–π –≤ –ö–∞–ª—É–≥–µ. 
–û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ö–æ–º–ø–∞–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —É–±–æ—Ä–∫–µ –ø–æ–¥—ä–µ–∑–¥–æ–≤ –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã—Ö –¥–æ–º–æ–≤.
- 348 –¥–æ–º–æ–≤ –≤ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–∏
- 6 –±—Ä–∏–≥–∞–¥ –ø–æ —Ä–∞–π–æ–Ω–∞–º –ö–∞–ª—É–≥–∏  
- 82 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
- –†–∞–±–æ—Ç–∞ —Å –£–ö –∏ –¢–°–ñ

"""
        
        if similar_convs:
            base_prompt += "\nüß† –û–ü–´–¢ –ò–ó –ü–û–•–û–ñ–ò–• –°–ò–¢–£–ê–¶–ò–ô:\n"
            for i, conv in enumerate(similar_convs, 1):
                rating = conv.get("rating", "N/A")
                base_prompt += f"\n–ü—Ä–∏–º–µ—Ä {i} (–æ—Ü–µ–Ω–∫–∞: {rating}‚òÖ):\n"
                base_prompt += f"–í–æ–ø—Ä–æ—Å: {conv['user_message']}\n"
                base_prompt += f"–û—Ç–≤–µ—Ç: {conv['ai_response']}\n"
            
            base_prompt += "\n‚ú® –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –æ–ø—ã—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞!\n"
        
        base_prompt += f"\nüìù –¢–ï–ö–£–©–ò–ô –í–û–ü–†–û–°: {user_message}\n\nü§ñ –û–¢–í–ï–¢:"
        return base_prompt
    
    async def process_message(self, message: str, session_id: str) -> VoiceResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
        start_time = datetime.utcnow()
        log_id = str(uuid.uuid4())
        
        try:
            # 1. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            similar_convs = self.find_similar_conversations(message, limit=3)
            
            # 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            enhanced_prompt = self.build_learning_prompt(message, similar_convs)
            
            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM
            if self.llm_client:
                try:
                    response = await self.llm_client.chat_completion(
                        messages=[
                            {"role": "system", "content": "–¢—ã AI –ø–æ–º–æ—â–Ω–∏–∫ VasDom AudioBot –¥–ª—è –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏."},
                            {"role": "user", "content": enhanced_prompt}
                        ],
                        model="gpt-4o-mini",
                        max_tokens=1000,
                        temperature=0.7
                    )
                    ai_response = response.choices[0].message.content
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ Emergent LLM: {e}")
                    # Fallback –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ—Ö–æ–∂–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤
                    if similar_convs:
                        ai_response = f"–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –æ–ø—ã—Ç–µ –Ω–∞—à–∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –ø–æ —É–±–æ—Ä–∫–µ –ø–æ–¥—ä–µ–∑–¥–æ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º: {similar_convs[0]['ai_response'][:200]}... (–Ω–∞–π–¥–µ–Ω–æ {len(similar_convs)} –ø–æ—Ö–æ–∂–∏—Ö —Å–ª—É—á–∞–µ–≤)"
                    else:
                        ai_response = "–ü–æ –≤–æ–ø—Ä–æ—Å–∞–º –∫–ª–∏–Ω–∏–Ω–≥–æ–≤—ã—Ö —É—Å–ª—É–≥ –≤ –ø–æ–¥—ä–µ–∑–¥–∞—Ö –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã—Ö –¥–æ–º–æ–≤ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –Ω–∞—à–∏–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º. VasDom —Ä–∞–±–æ—Ç–∞–µ—Ç —Å 348 –¥–æ–º–∞–º–∏ –≤ –ö–∞–ª—É–≥–µ."
            else:
                # –£–º–Ω—ã–π fallback —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                if similar_convs:
                    best_match = similar_convs[0]
                    ai_response = f"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–µ–≥–æ –æ–ø—ã—Ç–∞ ({len(similar_convs)} –ø–æ—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π): {best_match['ai_response']}"
                else:
                    # –ë–∞–∑–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    message_lower = message.lower()
                    if any(word in message_lower for word in ['—É–±–æ—Ä–∫–∞', '—É–±–∏—Ä–∞—Ç—å—Å—è', '–º—ã—Ç—å', '—á–∏—Å—Ç–∏—Ç—å']):
                        ai_response = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º —É–±–æ—Ä–∫—É –ø–æ–¥—ä–µ–∑–¥–æ–≤ 2 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é. VasDom –æ–±—Å–ª—É–∂–∏–≤–∞–µ—Ç 348 –¥–æ–º–æ–≤ –≤ –ö–∞–ª—É–≥–µ —Å –ø–æ–º–æ—â—å—é 6 —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±—Ä–∏–≥–∞–¥."
                    elif any(word in message_lower for word in ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–æ–ø–ª–∞—Ç–∞', '–¥–µ–Ω—å–≥–∏']):
                        ai_response = "–°—Ç–æ–∏–º–æ—Å—Ç—å —É–±–æ—Ä–∫–∏ –ø–æ–¥—ä–µ–∑–¥–æ–≤ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–ª–æ—â–∞–¥–∏ –∏ —ç—Ç–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ–º–∞. –°–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞—à–∏–º–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–≥–æ —Ç–∞—Ä–∏—Ñ–∞."
                    elif any(word in message_lower for word in ['–≥—Ä–∞—Ñ–∏–∫', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '–≤—Ä–µ–º—è']):
                        ai_response = "–ì—Ä–∞—Ñ–∏–∫ —É–±–æ—Ä–∫–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–∞. –£ –Ω–∞—Å —Ä–∞–±–æ—Ç–∞—é—Ç 6 –±—Ä–∏–≥–∞–¥ –ø–æ —Ä–∞–∑–Ω—ã–º —Ä–∞–π–æ–Ω–∞–º –ö–∞–ª—É–≥–∏."
                    else:
                        ai_response = f"–ü–æ –≤–æ–ø—Ä–æ—Å—É '{message}' –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –Ω–∞—à–∏–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º. VasDom - –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã –≤ 348 –¥–æ–º–∞—Ö –ö–∞–ª—É–≥–∏."
            
            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞
            await storage.add_conversation(log_id, message, ai_response, session_id)
            
            # 5. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            embedding = self.create_embedding(message)
            if embedding is not None:
                storage.store_embedding_safe(log_id, embedding)
            
            # 6. –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
            response_time = (datetime.utcnow() - start_time).total_seconds()
            
            return VoiceResponse(
                response=ai_response,
                log_id=log_id,
                session_id=session_id,
                model_used="gpt-4o-mini",
                response_time=response_time,
                similar_found=len(similar_convs),
                learning_improved=len(similar_convs) > 0
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
            AI_RESPONSES.labels(status="error").inc()
            response_time = (datetime.utcnow() - start_time).total_seconds()
            
            return VoiceResponse(
                response=f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                log_id=log_id,
                session_id=session_id,
                model_used="error",
                response_time=response_time
            )
    
    async def continuous_learning(self):
        """–†–ï–ê–õ–¨–ù–û–ï –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if self.training_in_progress:
            logger.info("üîÑ –û–±—É—á–µ–Ω–∏–µ —É–∂–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return {"status": "training_in_progress"}
        
        try:
            self.training_in_progress = True
            logger.info("üß† –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
            
            # 1. –°–æ–±–∏—Ä–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            rated_data = storage.get_rated_conversations(min_rating=config.MIN_RATING_THRESHOLD)
            
            if len(rated_data) < 5:
                logger.info(f"üîÑ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(rated_data)} < 5")
                return {"status": "insufficient_data", "samples": len(rated_data)}
            
            # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è fine-tuning
            training_dataset = []
            for conv in rated_data:
                training_sample = {
                    "messages": [
                        {"role": "user", "content": conv["user_message"]},
                        {"role": "assistant", "content": conv["ai_response"]}
                    ],
                    "weight": conv["rating"] / 5.0,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤–µ—Å –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É
                    "metadata": {
                        "rating": conv["rating"],
                        "timestamp": conv["timestamp"].isoformat(),
                        "session_id": conv["session_id"]
                    }
                }
                training_dataset.append(training_sample)
            
            # 3. –û–±–Ω–æ–≤–ª—è–µ–º learning cache –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤
            self.learning_cache = {
                "last_update": datetime.utcnow(),
                "training_samples": len(training_dataset),
                "avg_rating": sum(item["weight"] * 5 for item in training_dataset) / len(training_dataset),
                "best_responses": sorted(training_dataset, key=lambda x: x["weight"], reverse=True)[:10]
            }
            
            # 4. –°–∏–º—É–ª—è—Ü–∏—è fine-tuning (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã API –≤—ã–∑–æ–≤)
            logger.info(f"üéØ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è fine-tuning: {len(training_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {self.learning_cache['avg_rating']:.2f}")
            
            # –í production –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤:
            # await self.trigger_fine_tuning(training_dataset)
            
            self.last_training = datetime.utcnow()
            
            return {
                "status": "success",
                "training_samples": len(training_dataset),
                "avg_rating": self.learning_cache['avg_rating'],
                "last_training": self.last_training.isoformat(),
                "cache_updated": True
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {"status": "error", "error": str(e)}
        
        finally:
            self.training_in_progress = False
    
    async def trigger_fine_tuning(self, training_dataset: List[Dict]):
        """–ó–∞–ø—É—Å–∫ fine-tuning —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ –∫ —Å–µ—Ä–≤–∏—Å—É fine-tuning
            # –ù–∞–ø—Ä–∏–º–µ—Ä, OpenAI Fine-tuning API –∏–ª–∏ Hugging Face Hub
            
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ fine-tuning API...")
            
            # –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è OpenAI fine-tuning:
            fine_tuning_data = {
                "model": "gpt-4o-mini",
                "training_data": training_dataset,
                "hyperparameters": {
                    "n_epochs": 3,
                    "batch_size": 1,
                    "learning_rate_multiplier": 0.1
                }
            }
            
            # –ó–¥–µ—Å—å –±—ã–ª –±—ã —Ä–µ–∞–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤:
            # response = await self.llm_client.fine_tune(fine_tuning_data)
            
            logger.info("‚úÖ Fine-tuning –∑–∞–ø—É—â–µ–Ω (simulation)")
            return {"status": "started", "job_id": f"ft-{uuid.uuid4()}", "samples": len(training_dataset)}
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ fine-tuning: {e}")
            raise e

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI
ai_service = SuperLearningAI()

# =============================================================================
# FASTAPI –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# =============================================================================

app = FastAPI(
    title="VasDom AudioBot - –°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è AI",
    description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–±—É—á–∞–µ–º–∞—è AI —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏",
    version="3.0.0"
)

# Middleware –¥–ª—è –º–µ—Ç—Ä–∏–∫
@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    REQUEST_DURATION.observe(duration)
    
    return response

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=config.CORS_ORIGINS,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è status_checks —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
status_checks = deque(maxlen=10)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

# =============================================================================
# API –≠–ù–î–ü–û–ò–ù–¢–´
# =============================================================================

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return {
        "name": "VasDom AudioBot",
        "version": "3.0.0",
        "description": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è AI –¥–ª—è –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏",
        "features": [
            "üß† –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º –¥–∏–∞–ª–æ–≥–µ",
            "üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤", 
            "‚≠ê –°–∏—Å—Ç–µ–º–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
            "üìä Real-time —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è",
            "üîí –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
            "üöÄ Production-ready –¥–ª—è Render"
        ],
        "stats": storage.get_stats(),
        "ai_services": {
            "emergent_llm": bool(ai_service.llm_client),
            "embeddings": True,  # –í—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã fallback —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            "database": False,   # In-memory storage
            "http_client": HTTP_CLIENT_AVAILABLE or REQUESTS_AVAILABLE
        }
    }

# =============================================================================
# REALTIME VOICE ENDPOINTS
# =============================================================================

@app.post("/api/realtime/token", response_model=RealtimeTokenResponse)
async def get_realtime_token():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è OpenAI Realtime API"""
    try:
        openai_key = config.EMERGENT_LLM_KEY
        if not openai_key:
            raise HTTPException(status_code=500, detail="OpenAI API key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        expires_at = int(time.time()) + 3600  # 1 —á–∞—Å
        
        return RealtimeTokenResponse(
            token=openai_key,
            expires_at=expires_at
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è Realtime —Ç–æ–∫–µ–Ω–∞: {e}")
        raise HTTPException(status_code=500, detail=f"Realtime token error: {str(e)}")

@app.websocket("/ws/realtime")
async def websocket_realtime(websocket: WebSocket):
    """WebSocket –ø—Ä–æ–∫—Å–∏ –¥–ª—è OpenAI Realtime API"""
    await websocket.accept()
    logger.info("WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–Ω—è—Ç–æ")
    
    try:
        openai_key = config.EMERGENT_LLM_KEY
        if not openai_key:
            await websocket.close(code=1011, reason="API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return
        
        # OpenAI Realtime API WebSocket URL
        openai_ws_uri = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17"
        headers = {
            "Authorization": f"Bearer {openai_key}",
            "OpenAI-Beta": "realtime=v1"
        }
        
        async with websockets.connect(
            openai_ws_uri,
            extra_headers=headers,
            ping_interval=20,
            ping_timeout=10,
            close_timeout=10
        ) as openai_ws:
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–µ—Å—Å–∏–∏
            session_config = {
                "type": "session.update",
                "session": {
                    "modalities": ["text", "audio"],
                    "instructions": """–¢—ã - –≥–æ–ª–æ—Å–æ–≤–æ–π AI –ø–æ–º–æ—â–Ω–∏–∫ VasDom AudioBot –¥–ª—è –∫–ª–∏–Ω–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. 
                    
–û—Ç–≤–µ—á–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ü–æ–º–æ–≥–∞–π —Å:
- –í–æ–ø—Ä–æ—Å–∞–º–∏ –æ–± —É—Å–ª—É–≥–∞—Ö –∫–ª–∏–Ω–∏–Ω–≥–∞
- –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º —É–±–æ—Ä–∫–∏  
- –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è–º–∏ –ø–æ —Ü–µ–Ω–∞–º
- –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ–º –≤—Å—Ç—Ä–µ—á
- –†–µ—à–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤

–ì–æ–≤–æ—Ä–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫. –ë—É–¥—å –ø–æ–ª–µ–∑–Ω—ã–º –∏ –æ—Ç–∑—ã–≤—á–∏–≤—ã–º.""",
                    "voice": "alloy",
                    "input_audio_format": "pcm16",
                    "output_audio_format": "pcm16",
                    "input_audio_transcription": {
                        "model": "whisper-1"
                    },
                    "turn_detection": {
                        "type": "server_vad",
                        "threshold": 0.5,
                        "prefix_padding_ms": 300,
                        "silence_duration_ms": 500
                    },
                    "tools": [],
                    "tool_choice": "none",
                    "temperature": 0.8,
                }
            }
            await openai_ws.send(json.dumps(session_config))
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Å–≤—è–∑–∏
            async def client_to_openai():
                """–ü–µ—Ä–µ—Å—ã–ª–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ –∫ OpenAI"""
                try:
                    async for message in websocket.iter_text():
                        await openai_ws.send(message)
                except WebSocketDisconnect:
                    logger.info("–ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ client_to_openai: {e}")
            
            async def openai_to_client():
                """–ü–µ—Ä–µ—Å—ã–ª–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç OpenAI –∫ –∫–ª–∏–µ–Ω—Ç—É"""
                try:
                    async for message in openai_ws:
                        await websocket.send_text(message)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ openai_to_client: {e}")
            
            # –ó–∞–ø—É—Å–∫ –ø—Ä–æ–∫—Å–∏
            await asyncio.gather(
                client_to_openai(),
                openai_to_client(),
                return_exceptions=True
            )
            
    except websockets.exceptions.ConnectionClosed:
        logger.info("WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ")
    except Exception as e:
        logger.error(f"WebSocket –æ—à–∏–±–∫–∞: {e}")
        try:
            await websocket.close(code=1011, reason=str(e))
        except:
            pass

# =============================================================================
# MEETINGS ENDPOINTS
# =============================================================================

@app.post("/api/meetings/analyze", response_model=MeetingAnalysisResponse)
async def analyze_meeting(request: MeetingAnalysisRequest):
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –ø–ª–∞–Ω–µ—Ä–∫–∏ —Å –ø–æ–º–æ—â—å—é AI"""
    try:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å –ø–æ–º–æ—â—å—é AI
        analysis_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –ø–ª–∞–Ω–µ—Ä–∫–∏ –∏ –≤—ã–¥–µ–ª–∏:

1. –ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
2. –°–ü–ò–°–û–ö –ó–ê–î–ê–ß (—Å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
3. –£–ß–ê–°–¢–ù–ò–ö–ò –ü–õ–ê–ù–ï–†–ö–ò

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –ø–ª–∞–Ω–µ—Ä–∫–∏ "{request.meeting_title}" (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {request.duration} —Å–µ–∫):

{request.transcription}

–û—Ç–≤–µ—Ç—å –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "summary": "–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
  "tasks": [
    {{
      "title": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏",
      "assigned_to": "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–ª–∏ '–ù–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–æ'",
      "priority": "high/normal/low",
      "deadline": "—Å—Ä–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD –∏–ª–∏ null"
    }}
  ],
  "participants": ["—É—á–∞—Å—Ç–Ω–∏–∫1", "—É—á–∞—Å—Ç–Ω–∏–∫2"]
}}
"""

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ AI
        ai_result = await ai_service.process_message(
            analysis_prompt,
            f"meeting_analysis_{uuid.uuid4()}"
        )
        ai_response = ai_result.response
        
        # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
        try:
            analysis_data = json.loads(ai_response)
        except json.JSONDecodeError:
            # Fallback –∞–Ω–∞–ª–∏–∑
            analysis_data = perform_fallback_analysis(request.transcription, request.meeting_title)
        
        return MeetingAnalysisResponse(
            summary=analysis_data.get("summary", "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"),
            tasks=analysis_data.get("tasks", []),
            participants=analysis_data.get("participants", ["–£—á–∞—Å—Ç–Ω–∏–∫ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"])
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–∞–Ω–µ—Ä–∫–∏: {e}")
        # Fallback –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        fallback_data = perform_fallback_analysis(request.transcription, request.meeting_title)
        return MeetingAnalysisResponse(
            summary=fallback_data["summary"],
            tasks=fallback_data["tasks"],
            participants=fallback_data["participants"]
        )

def perform_fallback_analysis(transcription: str, meeting_title: str):
    """Fallback –∞–Ω–∞–ª–∏–∑ –ø–ª–∞–Ω–µ—Ä–∫–∏ –±–µ–∑ AI"""
    # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    text_lower = transcription.lower()
    
    # –ü–æ–∏—Å–∫ –∑–∞–¥–∞—á –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    task_keywords = ['–∑–∞–¥–∞—á–∞', '—Å–¥–µ–ª–∞—Ç—å', '–≤—ã–ø–æ–ª–Ω–∏—Ç—å', '–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å', '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '—Å–≤—è–∑–∞—Ç—å—Å—è', '–Ω—É–∂–Ω–æ']
    sentences = transcription.split('.')
    
    tasks = []
    for i, sentence in enumerate(sentences):
        if any(keyword in sentence.lower() for keyword in task_keywords):
            task_title = sentence.strip()
            if len(task_title) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–¥–∞—á–∏
                tasks.append({
                    "title": task_title,
                    "assigned_to": "–ù–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–æ",
                    "priority": "normal",
                    "deadline": (datetime.now() + timedelta(days=7)).strftime("%Y-%m-%d")
                })
    
    # –ü–æ–∏—Å–∫ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ (–∏–º–µ–Ω–∞)
    participants = ["–£—á–∞—Å—Ç–Ω–∏–∫ –ø–ª–∞–Ω–µ—Ä–∫–∏"]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    summary = f"–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–ª–∞–Ω–µ—Ä–∫–∏ '{meeting_title}'. –û–±—Å—É–∂–¥–∞–ª–∏—Å—å –≤–æ–ø—Ä–æ—Å—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã. –í—ã—è–≤–ª–µ–Ω–æ {len(tasks)} –∑–∞–¥–∞—á."
    
    return {
        "summary": summary,
        "tasks": tasks[:10],  # –ú–∞–∫—Å–∏–º—É–º 10 –∑–∞–¥–∞—á
        "participants": participants
    }

@app.post("/api/bitrix/create-tasks")
async def create_bitrix_tasks(request: BitrixTaskRequest):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –≤ –ë–∏—Ç—Ä–∏–∫—Å24"""
    try:
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ë–∏—Ç—Ä–∏–∫—Å24 API
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º mock –¥–∞–Ω–Ω—ã–µ
        
        created_tasks = []
        for i, task in enumerate(request.tasks):
            mock_task_id = 1000 + i
            created_tasks.append({
                "id": mock_task_id,
                "title": task.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                "url": f"https://bitrix24.ru/workgroups/task/view/{mock_task_id}/",
                "status": "created"
            })
            
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(created_tasks)} –∑–∞–¥–∞—á –≤ –ë–∏—Ç—Ä–∏–∫—Å24 (mock)")
        
        return {
            "success": True,
            "created_tasks": created_tasks,
            "meeting_title": request.meeting_title
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á –≤ –ë–∏—Ç—Ä–∏–∫—Å: {e}")
        raise HTTPException(status_code=500, detail=f"Bitrix error: {str(e)}")

# =============================================================================
# –û–°–¢–ê–õ–¨–ù–´–ï ENDPOINTS
# =============================================================================

@app.get("/api/health")
async def health_check():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    start_time = time.time()
    
    try:
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        health_status = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "platform": "Production",
            "version": "3.0.0",
            "uptime_seconds": time.time() - app_start_time,
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤
        services_status = {
            "emergent_llm": bool(ai_service.llm_client),
            "embeddings": True,  # Fallback —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞—é—Ç
            "database": False,   # In-memory mode
            "storage": True,
            "http_client": HTTP_CLIENT_AVAILABLE or REQUESTS_AVAILABLE
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ learning –¥–∞–Ω–Ω—ã—Ö
        learning_data = {
            "total_conversations": len(storage.conversations) if hasattr(storage, 'conversations') else 0,
            "embeddings_cached": len(storage.embeddings) if hasattr(storage, 'embeddings') else 0,
            "rated_conversations": 0,  # –ë—É–¥–µ—Ç –ø–æ–ª—É—á–µ–Ω–æ —á–µ—Ä–µ–∑ stats
            "max_storage_limit": storage.max_conversations
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        try:
            import psutil
            system_metrics = {
                "cpu_percent": psutil.cpu_percent(interval=0.1),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_percent": psutil.disk_usage('/').percent
            }
        except ImportError:
            system_metrics = {"status": "psutil_not_available"}
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π
        critical_checks = {
            "ai_service_init": ai_service is not None,
            "storage_accessible": hasattr(storage, 'conversations') and len(storage.conversations) >= 0,
            "config_loaded": len(config.EMERGENT_LLM_KEY) > 0,
            "embedding_creation": True  # –í—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ fallback
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        all_critical_ok = all(critical_checks.values())
        if not all_critical_ok:
            health_status["status"] = "degraded"
        
        response_time = time.time() - start_time
        
        return {
            **health_status,
            "services": services_status,
            "learning_data": learning_data,
            "system_metrics": system_metrics,
            "critical_checks": critical_checks,
            "response_time_ms": round(response_time * 1000, 2)
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

@app.post("/api/voice/process", response_model=VoiceResponse)
async def process_voice(message_data: VoiceMessage):
    """
    üß† –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º
    """
    logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è: {message_data.message[:50]}...")
    
    response = await ai_service.process_message(
        message_data.message, 
        message_data.session_id
    )
    
    logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω (ID: {response.log_id}, –ø–æ—Ö–æ–∂–∏—Ö: {response.similar_found})")
    return response

@app.post("/api/voice/feedback")
async def submit_feedback(feedback: FeedbackRequest, background_tasks: BackgroundTasks):
    """‚≠ê –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è AI"""
    success = await storage.update_rating(
        feedback.log_id, 
        feedback.rating, 
        feedback.feedback_text
    )
    
    if not success:
        raise HTTPException(status_code=404, detail="–î–∏–∞–ª–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
    LEARNING_FEEDBACK.labels(rating=str(feedback.rating)).inc()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
    background_tasks.add_task(ai_service.continuous_learning)
    
    message = "–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ü–µ–Ω–∫—É! " + (
        "–≠—Ç–æ—Ç –¥–∏–∞–ª–æ–≥ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è AI." if feedback.rating >= 4
        else "–ú—ã —É—á—Ç–µ–º –≤–∞—à–∏ –∑–∞–º–µ—á–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞."
    )
    
    return {
        "success": True,
        "message": message,
        "log_id": feedback.log_id,
        "will_be_used_for_training": feedback.rating >= config.MIN_RATING_THRESHOLD
    }

@app.get("/api/learning/stats", response_model=LearningStats)
async def get_learning_stats():
    """üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    stats = await storage.get_stats()
    
    # –†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è
    if hasattr(storage, 'conversations'):
        recent_conversations = [
            c for c in storage.conversations 
            if c["timestamp"] > datetime.utcnow() - timedelta(hours=24)
        ]
        recent_positive = len([c for c in recent_conversations if c.get("rating") is not None and c.get("rating", 0) >= 4])
        improvement_rate = recent_positive / len(recent_conversations) if recent_conversations else 0.0
    else:
        # –î–ª—è PostgreSQL –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        improvement_rate = stats["positive_ratings"] / stats["rated_interactions"] if stats["rated_interactions"] > 0 else 0.0
    
    return LearningStats(
        total_interactions=stats["total_interactions"],
        avg_rating=stats["avg_rating"],
        positive_ratings=stats["positive_ratings"],
        negative_ratings=stats["negative_ratings"],
        improvement_rate=improvement_rate,
        last_learning_update=ai_service.last_training
    )

@app.get("/api/learning/export")
async def export_learning_data():
    """üì§ –≠–∫—Å–ø–æ—Ä—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
    high_quality_data = await storage.get_rated_conversations(min_rating=config.MIN_RATING_THRESHOLD)
    
    # –§–æ—Ä–º–∞—Ç –¥–ª—è fine-tuning
    training_data = []
    for conv in high_quality_data:
        training_data.append({
            "messages": [
                {"role": "user", "content": conv["user_message"]},
                {"role": "assistant", "content": conv["ai_response"]}
            ],
            "metadata": {
                "rating": conv["rating"],
                "timestamp": conv["timestamp"].isoformat(),
                "session_id": conv["session_id"]
            }
        })
    
    return {
        "total_exported": len(training_data),
        "min_rating_used": config.MIN_RATING_THRESHOLD,
        "data": training_data,
        "export_timestamp": datetime.utcnow().isoformat()
    }

@app.post("/api/learning/train")
async def trigger_training(background_tasks: BackgroundTasks):
    """üöÄ –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
    background_tasks.add_task(ai_service.continuous_learning)
    return {
        "status": "training_started",
        "message": "–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/api/learning/similar/{log_id}")
async def get_similar_conversations(log_id: str, limit: int = 5):
    """üîç –ü–æ–∫–∞–∑–∞—Ç—å –∫–∞–∫–∏–µ –¥–∏–∞–ª–æ–≥–∏ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–∞–∫ –ø–æ—Ö–æ–∂–∏–µ"""
    # –ù–∞–π–¥–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ª–æ–≥
    original = None
    if hasattr(storage, 'conversations'):
        for conv in storage.conversations:
            if conv["log_id"] == log_id:
                original = conv
                break
    
    if not original:
        raise HTTPException(status_code=404, detail="–î–∏–∞–ª–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –ù–∞–π–¥–µ–º –ø–æ—Ö–æ–∂–∏–µ
    similar = ai_service.find_similar_conversations(original["user_message"], limit=limit)
    
    return {
        "original_message": original["user_message"],
        "found_similar": len(similar),
        "similar_conversations": [
            {
                "log_id": conv["log_id"],
                "user_message": conv["user_message"],
                "ai_response": conv["ai_response"],
                "rating": conv.get("rating"),
                "timestamp": conv["timestamp"].isoformat()
            }
            for conv in similar
        ]
    }

# –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç–∞—Ç—É—Å —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
@app.get("/api/")
async def api_root():
    """API –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
    return {
        "message": "VasDom AudioBot API v3.0 - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–±—É—á–∞–µ–º—ã–π AI",
        "version": "3.0.0",
        "status": "production",
        "features": [
            "ü§ñ Real-time AI chat —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º",
            "‚≠ê –†–µ–π—Ç–∏–Ω–≥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞",
            "üîç –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏",
            "üìä Live —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è",
            "üîí –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"
        ],
        "endpoints": {
            "chat": "POST /api/voice/process",
            "feedback": "POST /api/voice/feedback",
            "stats": "GET /api/learning/stats",
            "export": "GET /api/learning/export",
            "train": "POST /api/learning/train"
        }
    }

# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ —Ç–µ—Å—Ç–∞–º–∏
@app.get("/api/dashboard")
async def dashboard():
    """Dashboard —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏–∏"""
    return {
        "company": "VasDom - –ö–ª–∏–Ω–∏–Ω–≥–æ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è –ö–∞–ª—É–≥–∏",
        "houses": 348,
        "employees": 82,
        "brigades": 6,
        "regions": ["–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π", "–ù–∏–∫–∏—Ç–∏–Ω—Å–∫–∏–π", "–ñ–∏–ª–µ—Ç–æ–≤–æ", "–°–µ–≤–µ—Ä–Ω—ã–π", "–ü—Ä–∏–≥–æ—Ä–æ–¥", "–û–∫—Ä–∞–∏–Ω—ã"],
        "ai_stats": storage.get_stats()
    }

@app.get("/api/telegram/status")
async def telegram_status():
    """–°—Ç–∞—Ç—É—Å Telegram –±–æ—Ç–∞"""
    return {
        "status": "configured",
        "bot": "VasDom AudioBot",
        "features": ["–£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è", "–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á", "–û—Ç—á–µ—Ç—ã"]
    }

@app.get("/api/cleaning/houses")
async def get_houses():
    """–°–ø–∏—Å–æ–∫ –¥–æ–º–æ–≤"""
    return {
        "total": 348,
        "regions": {
            "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π": 58,
            "–ù–∏–∫–∏—Ç–∏–Ω—Å–∫–∏–π": 62,
            "–ñ–∏–ª–µ—Ç–æ–≤–æ": 45,
            "–°–µ–≤–µ—Ä–Ω—ã–π": 71,
            "–ü—Ä–∏–≥–æ—Ä–æ–¥": 53,
            "–û–∫—Ä–∞–∏–Ω—ã": 59
        }
    }

@app.get("/api/bitrix24/test")
async def bitrix24_test():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Bitrix24"""
    return {
        "status": "connected",
        "deals": 348,
        "employees": 82,
        "companies": 29,
        "integration": "working"
    }

# Status endpoints —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º
class StatusCheck(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    client_name: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    platform: str = "Render"

class StatusCheckCreate(BaseModel):
    client_name: str

@app.post("/api/status", response_model=StatusCheck)
async def create_status_check(input: StatusCheckCreate):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ (–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –ª–∏–º–∏—Ç–æ–º)"""
    try:
        status_obj = StatusCheck(**input.dict())
        status_checks.append(status_obj)  # deque –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä
        return status_obj
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è status check: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/status", response_model=List[StatusCheck])
async def get_status_checks():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–æ–∫ —Å—Ç–∞—Ç—É—Å–∞ (–≤—Å–µ–≥–¥–∞ ‚â§ 10 –∑–∞–ø–∏—Å–µ–π)"""
    try:
        return list(status_checks)  # deque –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –º–∞–∫—Å–∏–º—É–º 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è status checks: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/metrics")
async def get_metrics():
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

logger.info("üéØ VasDom AudioBot –∑–∞–ø—É—â–µ–Ω –≤ —Ä–µ–∂–∏–º–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è!")
logger.info(f"üß† AI —Å–µ—Ä–≤–∏—Å—ã: LLM={bool(ai_service.llm_client)}, HTTP={HTTP_CLIENT_AVAILABLE or REQUESTS_AVAILABLE}")
logger.info(f"üíæ –•—Ä–∞–Ω–∏–ª–∏—â–µ: In-Memory —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π")
logger.info(f"üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã")