"""
–ú–æ–¥—É–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI –º–æ–¥–µ–ª–∏
–í—ã–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ä—ã user_message/ai_response –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º
"""
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from sqlalchemy import and_, func
import logging

from app.config.database import SessionLocal
from app.models.database import VoiceLogDB, TrainingDatasetDB
from app.config.settings import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()

class TrainingDataExporter:
    """–≠–∫—Å–ø–æ—Ä—Ç–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, min_rating: int = None):
        self.min_rating = min_rating or settings.MIN_RATING_THRESHOLD
        self.export_dir = "training/data"
        os.makedirs(self.export_dir, exist_ok=True)
    
    async def export_training_data(
        self,
        days_back: int = 30,
        max_samples: int = 10000,
        output_filename: Optional[str] = None
    ) -> Dict:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL
        
        Args:
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤
            output_filename: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            
        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —ç–∫—Å–ø–æ—Ä—Ç–µ
        """
        try:
            async with SessionLocal() as db:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –≤—ã–±–æ—Ä–∫–∏
                cutoff_date = datetime.utcnow() - timedelta(days=days_back)
                
                # –ó–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–±–æ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É
                query = db.query(VoiceLogDB).filter(
                    and_(
                        VoiceLogDB.created_at >= cutoff_date,
                        VoiceLogDB.rating >= self.min_rating,
                        VoiceLogDB.user_message.isnot(None),
                        VoiceLogDB.ai_response.isnot(None)
                    )
                ).order_by(VoiceLogDB.created_at.desc()).limit(max_samples)
                
                logs = await query.all()
                
                # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É)
                total_query = db.query(func.count(VoiceLogDB.id)).filter(
                    VoiceLogDB.created_at >= cutoff_date
                )
                total_count = await total_query.scalar()
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                if not output_filename:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    output_filename = f"training_data_{timestamp}.jsonl"
                
                output_path = os.path.join(self.export_dir, output_filename)
                
                # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSONL —Ñ–æ—Ä–º–∞—Ç
                exported_count = 0
                with open(output_path, 'w', encoding='utf-8') as f:
                    for log in logs:
                        training_sample = {
                            "messages": [
                                {"role": "user", "content": log.user_message},
                                {"role": "assistant", "content": log.ai_response}
                            ],
                            "metadata": {
                                "log_id": log.id,
                                "rating": log.rating,
                                "session_id": log.session_id,
                                "created_at": log.created_at.isoformat(),
                                "model_used": log.model_used
                            }
                        }
                        f.write(json.dumps(training_sample, ensure_ascii=False) + '\n')
                        exported_count += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤ –ë–î
                dataset = TrainingDatasetDB(
                    dataset_name=f"training_export_{datetime.now().strftime('%Y%m%d')}",
                    version="1.0",
                    total_samples=exported_count,
                    min_rating_threshold=self.min_rating,
                    jsonl_path=output_path,
                    status="completed"
                )
                db.add(dataset)
                await db.commit()
                
                export_info = {
                    "success": True,
                    "total_samples": total_count,
                    "filtered_samples": exported_count,
                    "min_rating_threshold": self.min_rating,
                    "export_path": output_path,
                    "dataset_id": dataset.id,
                    "created_at": datetime.utcnow().isoformat(),
                    "period_days": days_back
                }
                
                logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {exported_count} –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ {total_count} –∑–∞–ø–∏—Å–µ–π")
                return export_info
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "total_samples": 0,
                "filtered_samples": 0
            }
    
    async def validate_export_data(self, file_path: str) -> Dict:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        try:
            if not os.path.exists(file_path):
                return {"valid": False, "error": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            line_count = 0
            valid_samples = 0
            errors = []
            
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line_count += 1
                    try:
                        data = json.loads(line.strip())
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                        if "messages" not in data:
                            errors.append(f"–°—Ç—Ä–æ–∫–∞ {line_num}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'messages'")
                            continue
                        
                        messages = data["messages"]
                        if len(messages) != 2:
                            errors.append(f"–°—Ç—Ä–æ–∫–∞ {line_num}: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–æ–≤–Ω–æ 2 —Å–æ–æ–±—â–µ–Ω–∏—è")
                            continue
                        
                        if messages[0]["role"] != "user" or messages[1]["role"] != "assistant":
                            errors.append(f"–°—Ç—Ä–æ–∫–∞ {line_num}: –Ω–µ–≤–µ—Ä–Ω—ã–µ —Ä–æ–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π")
                            continue
                        
                        valid_samples += 1
                        
                    except json.JSONDecodeError as e:
                        errors.append(f"–°—Ç—Ä–æ–∫–∞ {line_num}: –æ—à–∏–±–∫–∞ JSON - {str(e)}")
            
            return {
                "valid": len(errors) == 0,
                "total_lines": line_count,
                "valid_samples": valid_samples,
                "errors": errors[:10],  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –æ—à–∏–±–æ–∫
                "error_count": len(errors)
            }
            
        except Exception as e:
            return {"valid": False, "error": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}"}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è CLI –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def export_logs_cli(min_rating: int = 4, days_back: int = 30, max_samples: int = 10000):
    """CLI —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –ª–æ–≥–æ–≤"""
    exporter = TrainingDataExporter(min_rating=min_rating)
    result = await exporter.export_training_data(
        days_back=days_back,
        max_samples=max_samples
    )
    
    if result["success"]:
        print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω!")
        print(f"üìä –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {result['filtered_samples']} –∏–∑ {result['total_samples']} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìÅ –§–∞–π–ª: {result['export_path']}")
        print(f"‚≠ê –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {result['min_rating_threshold']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {result['error']}")
    
    return result

if __name__ == "__main__":
    import asyncio
    import argparse
    
    parser = argparse.ArgumentParser(description="–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI –º–æ–¥–µ–ª–∏")
    parser.add_argument("--min-rating", type=int, default=4, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ (1-5)")
    parser.add_argument("--days-back", type=int, default=30, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥")
    parser.add_argument("--max-samples", type=int, default=10000, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    args = parser.parse_args()
    
    asyncio.run(export_logs_cli(
        min_rating=args.min_rating,
        days_back=args.days_back,
        max_samples=args.max_samples
    ))