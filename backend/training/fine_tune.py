"""
–ú–æ–¥—É–ª—å –¥–æ–æ–±—É—á–µ–Ω–∏—è AI –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç transformers –¥–ª—è fine-tuning –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""
import os
import json
import torch
import logging
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import argparse

try:
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    )
    from datasets import Dataset
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.")

from app.config.settings import get_settings
from app.config.database import SessionLocal
from app.models.database import TrainingDatasetDB, ModelMetricsDB
from sqlalchemy import select

logger = logging.getLogger(__name__)
settings = get_settings()

class ModelFineTuner:
    """–ö–ª–∞—Å—Å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, base_model_name: str = "microsoft/DialoGPT-medium"):
        self.base_model_name = base_model_name
        self.output_dir = settings.LOCAL_MODEL_PATH
        self.training_data_dir = "training/data"
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.training_data_dir).mkdir(parents=True, exist_ok=True)
        
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fine-tuner –¥–ª—è {base_model_name} –Ω–∞ {self.device}")
    
    def load_base_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        if not TRANSFORMERS_AVAILABLE:
            logger.error("transformers –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
            return False
        
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self.base_model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.base_model_name)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.model.to(self.device)
            
            logger.info("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {str(e)}")
            return False
    
    def prepare_dataset(self, jsonl_file_path: str) -> Optional[Dataset]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ JSONL —Ñ–∞–π–ª–∞
        
        Args:
            jsonl_file_path: –ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if not os.path.exists(jsonl_file_path):
                logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {jsonl_file_path}")
                return None
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            training_data = []
            with open(jsonl_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        messages = data.get("messages", [])
                        
                        if len(messages) >= 2:
                            user_msg = messages[0]["content"] 
                            assistant_msg = messages[1]["content"]
                            
                            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                            conversation = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_msg}\n–ü–æ–º–æ—â–Ω–∏–∫: {assistant_msg}"
                            training_data.append(conversation)
                            
                    except json.JSONDecodeError as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {str(e)}")
                        continue
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(training_data)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            if len(training_data) == 0:
                logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return None
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            def tokenize_function(examples):
                return self.tokenizer(
                    examples["text"],
                    truncation=True,
                    padding=True,
                    max_length=512
                )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ Dataset
            dataset = Dataset.from_dict({"text": training_data})
            tokenized_dataset = dataset.map(tokenize_function, batched=True)
            
            logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {len(tokenized_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            return tokenized_dataset
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
            return None
    
    def fine_tune_model(
        self,
        dataset: Dataset,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 5e-5,
        save_steps: int = 500
    ) -> Dict:
        """
        –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            dataset: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            save_steps: –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            logger.info("üöÄ –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                overwrite_output_dir=True,
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size,
                warmup_steps=100,
                learning_rate=learning_rate,
                logging_steps=50,
                save_steps=save_steps,
                evaluation_strategy="no",  # –ü–æ–∫–∞ –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                remove_unused_columns=False,
                dataloader_num_workers=0,  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                report_to=[],  # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ wandb
            )
            
            # Data collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,  # Causal LM
                pad_to_multiple_of=8
            )
            
            # Trainer
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer,
            )
            
            # –û–±—É—á–µ–Ω–∏–µ
            start_time = datetime.now()
            train_result = trainer.train()
            end_time = datetime.now()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            
            training_duration = (end_time - start_time).total_seconds()
            
            result = {
                "success": True,
                "training_loss": train_result.training_loss,
                "num_epochs": num_epochs,
                "training_duration_seconds": training_duration,
                "model_path": self.output_dir,
                "timestamp": end_time.isoformat()
            }
            
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ –∑–∞ {training_duration:.1f} —Å–µ–∫")
            logger.info(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def update_training_dataset_status(
        self,
        dataset_id: int,
        status: str,
        training_logs: Optional[str] = None,
        model_weights_path: Optional[str] = None
    ):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –ë–î"""
        try:
            async with SessionLocal() as db:
                dataset = await db.query(TrainingDatasetDB).filter(
                    TrainingDatasetDB.id == dataset_id
                ).first()
                
                if dataset:
                    dataset.status = status
                    dataset.training_logs = training_logs
                    
                    if status == "training":
                        dataset.training_started_at = datetime.utcnow()
                    elif status == "completed":
                        dataset.training_completed_at = datetime.utcnow()
                        dataset.model_weights_path = model_weights_path
                    
                    await db.commit()
                    logger.info(f"–°—Ç–∞—Ç—É—Å –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_id} –æ–±–Ω–æ–≤–ª–µ–Ω –Ω–∞ {status}")
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")

async def fine_tune_from_export(
    jsonl_file_path: str,
    base_model: str = "microsoft/DialoGPT-medium",
    num_epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 5e-5
) -> Dict:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        jsonl_file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
        base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
        num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    """
    if not TRANSFORMERS_AVAILABLE:
        return {
            "success": False,
            "error": "transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
        }
    
    fine_tuner = ModelFineTuner(base_model_name=base_model)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    if not fine_tuner.load_base_model():
        return {
            "success": False,
            "error": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏"
        }
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset = fine_tuner.prepare_dataset(jsonl_file_path)
    if dataset is None:
        return {
            "success": False,
            "error": "–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"
        }
    
    # –î–æ–æ–±—É—á–µ–Ω–∏–µ
    result = fine_tuner.fine_tune_model(
        dataset=dataset,
        num_epochs=num_epochs,
        batch_size=batch_size,
        learning_rate=learning_rate
    )
    
    return result

if __name__ == "__main__":
    import asyncio
    
    parser = argparse.ArgumentParser(description="–î–æ–æ–±—É—á–µ–Ω–∏–µ AI –º–æ–¥–µ–ª–∏ VasDom AudioBot")
    parser.add_argument("--data", type=str, required=True, help="–ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument("--model", type=str, default="microsoft/DialoGPT-medium", help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    parser.add_argument("--epochs", type=int, default=3, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch-size", type=int, default=4, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--learning-rate", type=float, default=5e-5, help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
    
    args = parser.parse_args()
    
    async def main():
        print("üöÄ –ó–∞–ø—É—Å–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ VasDom AudioBot")
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {args.data}")
        print(f"ü§ñ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {args.model}")
        print(f"üìà –≠–ø–æ—Ö–∏: {args.epochs}, –ë–∞—Ç—á: {args.batch_size}, LR: {args.learning_rate}")
        
        result = await fine_tune_from_export(
            jsonl_file_path=args.data,
            base_model=args.model,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate
        )
        
        if result["success"]:
            print("‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {result['model_path']}")
            print(f"üìä Loss: {result.get('training_loss', 'N/A')}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {result['error']}")
    
    asyncio.run(main())